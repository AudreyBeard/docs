---
description: The Prompts Quickstart shows how to visualise and debug the execution flow of your LLM chains and pipelines
---

# Quickstart

<head>
  <title>Prompts Quickstart</title>
</head>

This Quickstart guide will walk you how to use [W&B Tracer](intro.md) to visualize and debug the execution flow of your LLM chains or pipelines.

<!-- This Quickstart guide will walk you how to use Weights & Biases (W&B) Prompts tools to visualise and debug the execution flow of your LLM chains or pipelines. -->

To get started, click the colab button below to try a Quickstart example. 

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](http://wandb.me/prompts-quickstart)

## Use Tracer with LangChain

With one line of code W&B Tracer will automatically and continuously log calls to a LangChain model, chain, or agent.


### 1. Import and initialize WandbTracer

Start wandb watching for calls to LangChain Models, Chains, or Agents. You can pass a dictionary with argument that `wandb.init()` accepts to `watch_all`, such as your project name, team name, run name etc:

```python
import wandb
from langchain.callbacks.wandb_tracer import WandbTracer

WandbTracer.watch_all({"project": "auto-gpt"})
```

Once the chain execution completes, any call to a LangChain object are logged automatically to the W&B Tracer. 

### 2. Set up your LangChain Agent

For this demo, we will use a LangChain Math agent:

```python
from langchain.llms import OpenAI
from langchain.agents import load_tools, initialize_agent, AgentType

llm = OpenAI(temperature=0)
tools = load_tools(["llm-math"], llm=llm)
math_agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION)
```

### 3. Make calls to your Agent

For every call to `math_agent`, the `WandbTracer` will log the full trace of every call to the LangChain Models, Chains, and Agents once the execution is complete. The parameters used to create all these objects are also logged:

```python
questions = [
    "Find the square root of 5.4.",
    "What is 3 divided by 7.34 raised to the power of pi?",
    "What is the sin of 0.47 radians, divided by the cube root of 27?"
]

for question in questions:
  try:
    answer = math_agent.run(question)
    print(answer)
  except Exception as e:
    print(e)
    pass
```

### 4. View the trace

Click on the Weights & Biases run link generated by `WandbTracer.watch_all` above to see the trace of your LLM.


### 5. Stop watching
When you're finished with your development, it's recommended to call `stop_watch()` to close the wandb process cleanly. 

```python
WandbTracer.stop_watch()
```

## Use Tracer with Any LLM Chain or Plug-In

To use Tracer with your own chains, plug-ins or pipelines, you first need to create traces using the `Span` and `TraceTree` data types directly.

### 1. Create a Span
A Span represents a unit of work:

```python
from wandb.sdk.data_types import trace_tree

span = trace_tree.Span(name="Example Span")
```

### 2. Add child Spans
Nest child Spans within the parent span:

```python
child_span_1 = trace_tree.Span(name="Child Span 1")
child_span_2 = trace_tree.Span(name="Child Span 2")
span.add_child_span(child_span_1)
span.add_child_span(child_span_2)
```

### 3. Add the inputs and outputs

Populate spans with the input and output data

```python
child_span_1.add_named_result({"input": "input_example_1"}, {"response": "response_example_1"})
child_span_2.add_named_result({"input": "input_example_2"}, {"response": "response_example_2"})
span.add_named_result({"input": "input_parent_example"}, {"response": "response_parent_example"})
```

### 4. Log the spans to Weights & Biases' Tracer 

This will allow you to visualize the Trace Table, Trace Timeline, and Model Architecture.

```python
import wandb 

trace = trace_tree.WBTraceTree(span)
run = wandb.init(project="auto-gpt")
run.log({"trace": trace})
run.finish()
```
### 5. View the trace
Click on the W&B run link that gets generated to see the trace of your LLM.