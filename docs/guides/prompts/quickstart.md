---
description: The Prompts Quickstart shows how to visualise and debug the execution flow of your LLM chains and pipelines
---

# Quickstart

<head>
  <title>Prompts Quickstart</title>
</head>


This Quickstart guide will walk you through using Weights & Biases (W&B) Prompts tools to visualise and debug the execution flow of your LLM chains or pipelines.

To get started, click the colab button below to try a Quickstart example. 

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](http://wandb.me/prompts-quickstart)

## Tracer
Tracer is a tool that enables you to:

- Visualize the execution flow of your LLM code
- Save your chain, plug-in or pipeline architecture
- Analyze inputs, outputs, and intermediate results

You can use Tracer for any LLM chaining, plug-in, or pipelining use case, either with your own LLM chaining implementation or with our integrations in popular LLM librarys such as LangChain.


### Using Tracer with LangChain

In this example, we'll show you how to use Tracer with LangChain. After running just one line of code, Tracer will automaticall and continuously log every call to a LangChain Model, Chain, or Agent without requiring any additional user input.

1. **Import and initialize WandbTracer** - Start wandb watching for calls to LangChain Models, Chains, or Agents. You can pass a dictionary with argument that `wandb.init()` accepts to `watch_all`, such as your project name, team name, run name etc:

```python
import wandb
from langchain.callbacks.wandb_tracer import WandbTracer

WandbTracer.watch_all({"project": "auto-gpt"})
```

From now on, any call to these langchain objects will be logged automatically to the Weights & Biases Tracer once the chain execution completes.

2. **Set up your LangChain Agent** - We'll use a simple Math agent here:

```python
from langchain.llms import OpenAI
from langchain.agents import load_tools, initialize_agent, AgentType

llm = OpenAI(temperature=0)
tools = load_tools(["llm-math"], llm=llm)
math_agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION)
```

3. **Make calls to your Agent** - For every call to `math_agent`, the `WandbTracer` will log the full trace of every call to the LangChain Models, Chains, and Agents once the execution is complete. The parameters used to create all these objects are also logged:

```python
questions = [
    "Find the square root of 5.4.",
    "What is 3 divided by 7.34 raised to the power of pi?",
    "What is the sin of 0.47 radians, divided by the cube root of 27?"
]

for question in questions:
  try:
    answer = math_agent.run(question)
    print(answer)
  except Exception as e:
    print(e)
    pass
```

4. **View the trace** - Click on the Weights & Biases run link generated by `WandbTracer.watch_all` above to see the trace of your LLM.


5. **Stop watching** - When you're finished with your development, it's recommended to call `stop_watch()` to close the wandb process cleanly. 

```python
WandbTracer.stop_watch()
```

### Using Tracer with Any LLM Chain or Plug-In

To use Tracer with your own chains, plug-ins or pipelines, you first need to manually create traces using the `Span` and `TraceTree` data types directly.

1. **Create a Span** - A Span represents a unit of work:

```python
from wandb.sdk.data_types import trace_tree

span = trace_tree.Span(name="Example Span")
```

2. **Add child Spans** - Neest child Spans within the parent span:

```python
child_span_1 = trace_tree.Span(name="Child Span 1")
child_span_2 = trace_tree.Span(name="Child Span 2")
span.add_child_span(child_span_1)
span.add_child_span(child_span_2)
```

3. **Add the inputs and outputs** - Populate spans with the input and output data

```python
child_span_1.add_named_result({"input": "input_example_1"}, {"response": "response_example_1"})
child_span_2.add_named_result({"input": "input_example_2"}, {"response": "response_example_2"})
span.add_named_result({"input": "input_parent_example"}, {"response": "response_parent_example"})
```

4. **Log the spans to Weights & Biases' Tracer** - This will allow you to visualize the Trace Table, Trace Timeline, and Model Architecture.

```python
import wandb 

trace = trace_tree.WBTraceTree(span)
run = wandb.init(project="auto-gpt")
run.log({"trace": trace})
run.finish()
```
5. **View the trace** - Click on the Weights & Biases run link that gets generated to see the trace of your LLM.