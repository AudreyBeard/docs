---
description: The Prompts Quickstart shows how to visualise and debug the execution flow of your LLM chains and pipelines
---

# Quickstart

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](http://wandb.me/prompts-quickstart)


<head>
  <title>Prompts Quickstart</title>
</head>

This Quickstart guide will walk you how to use [W&B Trace](intro.md) to visualize and debug calls to LangChain or any other LLM Chain.

<!-- This Quickstart guide will walk you how to use Weights & Biases (W&B) Prompts tools to visualise and debug the execution flow of your LLM chains or pipelines. -->


## Use Trace with LangChain

With one line of code W&B Trace will automatically and continuously log calls to a [LangChain Model](https://python.langchain.com/en/latest/modules/models.html), [Chain](https://python.langchain.com/en/latest/modules/chains.html), or [Agent](https://python.langchain.com/en/latest/modules/agents.html).

Follow the steps below to visualize and debug LangChain. For this demo, we will use a LangChain Math agent.

### 1. Import and initialize WandbTracer

First, import `WandbTracer` from `langchain.callbacks.wandb_tracer`.  Call the `watch_all()` method to make W&B start watching for calls to LangChain Models, Chains, or Agents.

```python
import wandb
from langchain.callbacks.wandb_tracer import WandbTracer

WandbTracer.watch_all({"project": "auto-gpt"})
```

You can optionally pass a dictionary with argument that `wandb.init()` accepts to `watch_all`. This includes a project name, team name, entity, and more. For more information about [`wandb.init`](../../ref/python/init.md), see the API Reference Guide.


Once the chain execution completes, any call to a LangChain object is logged automatically to the W&B Trace. 

### 2. Set up your LangChain Agent
Import an OpenAI Langchain Agent and create a math tool(function) with `load_tools`.  Next create a math agent with the [`initialize_agent`](https://python.langchain.com/en/latest/_modules/langchain/agents/initialize.html) method and pass the tool object to `initialize_agent`:

```python
from langchain.llms import OpenAI
from langchain.agents import load_tools, initialize_agent, AgentType

llm = OpenAI(temperature=0)
tools = load_tools(["llm-math"], llm=llm)
math_agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION)
```

### 3. Make calls to your Agent

Every call made to the agent (in this example, `math_agent`) is logged once the execution is complete.

The parameters used to create objects are also logged:

```python
questions = [
    "Find the square root of 5.4.",
    "What is 3 divided by 7.34 raised to the power of pi?",
    "What is the sin of 0.47 radians, divided by the cube root of 27?"
]

for question in questions:
  try:
    answer = math_agent.run(question)
    print(answer)
  except Exception as e:
    print(e)
    pass
```

### 4. View the trace

Click on the Weights & Biases [run](../runs/intro.md) link generated by `WandbTracer.watch_all` in the previous step. This will redirect you to your project workspace in the W&B App. 

Select a run you created to view the trace table, trace timeline and the model architecture of your LLM. 

![](/images/prompts/trace_timeline_detailed.png)




### 5. Stop watching
We recommend that you call `stop_watch()` to close all W&B processes when you are finished with your development.

```python
WandbTracer.stop_watch()
```



## Use Trace with Any LLM Chain or Plug-In

To use Trace with your own chains, plug-ins or pipelines, you first need to create traces using the `Span` and `TraceTree` data types. A _Span_ represents a unit of work.

### 1. Create a Span
First, create a span object. Import `trace_tree` from the `wandb.sdk.data_types`:

```python
from wandb.sdk.data_types import trace_tree

span = trace_tree.Span(name="Example Span")
```

### 2. Add child Spans
Nest child Spans within the parent span:

```python
child_span_1 = trace_tree.Span(name="Child Span 1")
child_span_2 = trace_tree.Span(name="Child Span 2")
span.add_child_span(child_span_1)
span.add_child_span(child_span_2)
```

### 3. Add the inputs and outputs

Populate spans with the input and output data

```python
child_span_1.add_named_result({"input": "input_example_1"}, {"response": "response_example_1"})
child_span_2.add_named_result({"input": "input_example_2"}, {"response": "response_example_2"})
span.add_named_result({"input": "input_parent_example"}, {"response": "response_parent_example"})
```

### 4. Log the spans to Weights & Biases' Trace 

This will allow you to visualize the Trace Table, Trace Timeline, and Model Architecture.

```python
import wandb 

trace = trace_tree.WBTraceTree(span)
run = wandb.init(project="auto-gpt")
run.log({"trace": trace})
run.finish()
```
### 5. View the trace
Click on the W&B run link that gets generated to see the trace of your LLM.