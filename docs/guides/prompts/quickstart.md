---
description: The Prompts Quickstart shows how to visualise and debug the execution flow of your LLM chains and pipelines
displayed_sidebar: default
---

# Prompts Quickstart

[**Try in a Colab Notebook here â†’**](http://wandb.me/prompts-quickstart)

<head>
  <title>Prompts Quickstart</title>
</head>

This Quickstart guide will walk you how to use [Trace](intro.md) to visualize and debug calls to LangChain, LlamaIndex or any other LLM Chain or Pipeline.

1. Langchain: Use `WandbTracer` to visualize and inspect the execution flow of your LLMs, analyze the inputs and outputs of your LLMs, view the intermediate results and securely store and manage your prompts and LLM chain configurations.

2. LlamaIndex: Use `WandbCallbackHandler` to keep track and visualize the trace (collection of events) maps, analyze the inputs and outputs of your LLM calls, upload and download indices and debug your retrieval and query engine.

3. Custom usage: Use W&B Prompts with your own chains and pipelines.


<!-- This Quickstart guide will walk you how to use W&B (W&B) Prompts tools to visualise and debug the execution flow of your LLM chains or pipelines. -->


## Use Trace with LangChain

:::info
**Versions** Please use `wandb >= 0.15.3` and `langchain >= 0.0.188` when using `WandbTracer` 

**LangChain:** Note that from `langchain >= 0.0.188` the W&B Prompts integration for LangChain can now be found in the `langchain` library. Please see the [LangChain documentation](https://python.langchain.com/en/latest/integrations/agent_with_wandb_tracing.html) for more.
:::

W&B Trace will continuously log calls to a [LangChain Model](https://python.langchain.com/en/latest/modules/models.html), [Chain](https://python.langchain.com/en/latest/modules/chains.html), or [Agent](https://python.langchain.com/en/latest/modules/agents.html).

Follow the steps below to visualize and debug a LangChain chain. For this demo, we will use a LangChain Math agent.

### 1. Import WandbTracer

First, import `WandbTracer` from `wandb.integration.langchain`.

```python
from wandb.integration.langchain import WandbTracer

wandb_config = {"project": "wandb_prompts_quickstart"}
```

You can optionally define a dictionary with arguments for `wandb.init()` that will later be passed to `WandbTracer`. This includes a project name, team name, entity, and more. For more information about [`wandb.init`](../../ref/python/init.md), see the API Reference Guide.


### 2. Set up your LangChain Agent
Import an OpenAI model and create a math tool with `load_tools`. Next create a math agent with the [`initialize_agent`](https://python.langchain.com/en/latest/_modules/langchain/agents/initialize.html) method and pass the tool and model objects to `initialize_agent`:

```python
from langchain.llms import OpenAI
from langchain.agents import load_tools, initialize_agent, AgentType

llm = OpenAI(temperature=0)
tools = load_tools(["llm-math"], llm=llm)
math_agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION)
```

### 3. Pass WandbTracer as a callback

Pass `WandbTracer` to the agent's `callbacks` argument in the `.run` method. This will enable every call made to the agent (in this example, `math_agent`) to be logged to W&B once the execution is complete.

The parameters used to create objects are also logged:

```python
questions = [
    "Find the square root of 5.4.",
    "What is 3 divided by 7.34 raised to the power of pi?",
    "What is the sin of 0.47 radians, divided by the cube root of 27?"
]

for question in questions:
  try:
    answer = math_agent.run(question, 
                            callbacks=[WandbTracer(wandb_config)])
    print(answer)
  except Exception as e:
    print(e)
    pass
```

If you had previously created a dictionary for your `wandb.init` arguments, you can pass it to `WandbTracer` here now as shown.

Once the chain execution completes, any call to your LangChain object will be logged to the W&B Trace. 

### 4. View the trace

Click on the W&B [run](../runs/intro.md) link generated by `WandbTracer.init` in the previous step. This will redirect you to your project workspace in the W&B App. 

Select a run you created to view the trace table, trace timeline and the model architecture of your LLM. 

![](/images/prompts/trace_timeline_detailed.png)




### 5. Stop watching
We recommend that you call `WandbTracer.finish` to close all W&B processes when you are finished with your development.

```python
WandbTracer.finish()
```


## Use Trace with LlamaIndex

:::info
**Versions** Please use `wandb >= 0.15.3` and `llama-index >= 0.6.21` when using `WandbCallbackHandler`.
:::

At the lowest level, LlamaIndex uses the concept of start/end events ([`CBEventTypes`](https://gpt-index.readthedocs.io/en/latest/reference/callbacks.html#llama_index.callbacks.CBEventType)) to keep a track of logs. Each event has some payload which provides information like, the query asked and the response generated by the LLM, or about the number of documents used to create N chunks, etc.

At a higher level, they have recently introduced the concept of [Callback Tracing](https://github.com/jerryjliu/llama_index/pull/3835) which builds a trace map of connected events. For example when you query over an index, under the hood, retrieval, LLM calls, etc. takes place.

`WandbCallbackHandler` provides an intuitive way to visualize and track this trace map. It captures the payload of the events and logs them to wandb. It also tracks necessary metadata like total token counts, prompt, context, etc.

Moreover, this callback can also be used to upload and download indices to/from W&B Artifacts for version controlling your indices.

### 1. Import WandbCallbackHandler

First import the `WandbCallbackHandler` and set it up.

```
from llama_index import ServiceContext
from llama_index.callbacks import CallbackManager, WandbCallbackHandler

# wandb.init args
run_args = dict(
    project="llamaindex",
)

wandb_callback = WandbCallbackHandler(run_args=run_args)

callback_manager = CallbackManager([wandb_callback])
service_context = ServiceContext.from_defaults(callback_manager=callback_manager)
```

### 2. Build an Index

We will build a simple index using a text file.

```
docs = SimpleDirectoryReader("path_to_dir").load_data()
index = GPTVectorStoreIndex.from_documents(docs, service_context=service_context)
```

We will then upload this index as a W&B artifacts. The string passed to the `index_name` is the created artifact. You can go to the artifacts tab on your W&B run page to check out the uploaded index.

```
wandb_callback.persist_index(index, index_name="simple_vector_store")
```

We will not download the index and use it to create a query engine. The method used to download the index, returns a [`StorageContext`](https://gpt-index.readthedocs.io/en/latest/reference/storage.html). Use this storage context to load the index into memory using different [loading functions](https://gpt-index.readthedocs.io/en/latest/reference/storage/indices_save_load.html).

:::info
For [`ComposableGraph`](https://gpt-index.readthedocs.io/en/latest/reference/query/query_engines/graph_query_engine.html) the root id for the index can be found in the artifact's metadata tab.
:::

```
from llama_index import load_index_from_storage

storage_context = wandb_callback.load_storage_context(artifact_url="<entity/project/index_name:version>")

index = load_index_from_storage(storage_context, service_context=service_context)
```

### 3. Query over an index

With the loaded index, start querying over your documents.

```
query_engine = index.as_query_engine()
response = query_engine.query("What did the author do growing up?")
```

### 4. View the trace

Click on the Weights and Biases run link generated while initializing the `WandbCallbackHandler` in the previous step. This will take you to your project workspace in the W&B App.

You will find a trace table and a trace timeline.

![](/images/prompts/llama_index_trace.png)

### 5. Stop tracking

When you are done tracking your traces, do this to safely close all W&B processes.

```
wandb_callback.finish()
```


## Use Trace with Any LLM Chain or Plug-In

When logging with Trace, a single run can have multiple calls to a LLM, Tool, Chain or Agent logged to it, there is no need to start a new run after each generation from your model, each call will be appended to the Trace Table.

To use Trace with your own chains, plug-ins or pipelines, you first need to create traces using the `Span` and `TraceTree` data types. A _Span_ represents a unit of work.

### 1. Create a Span
First, create a span object. Import `trace_tree` from the `wandb.sdk.data_types`:

```python
from wandb.sdk.data_types import trace_tree

# Root Span - Create a span for your high level agent
root_span = trace_tree.Span(name="Auto-GPT", 
  span_kind = trace_tree.SpanKind.AGENT)
```

Spans can be of type `AGENT`, `CHAIN`, `TOOL` or `LLM`

### 2. Add child Spans
Nest child Spans within the parent span so that they are nested and in the correct order in the Trace Timeline view. 

The following text code demonstrates how to create two child spans and one grandchild span:

```python
tool_span = trace_tree.Span(
  name="Tool 1", span_kind = trace_tree.SpanKind.TOOL
)

chain_span = trace_tree.Span(
  name="LLM CHAIN 1", span_kind = trace_tree.SpanKind.CHAIN
)

llm_span = trace_tree.Span(
  name="LLM 1", span_kind = trace_tree.SpanKind.LLM
)

chain_span.add_child_span(llm_span)
root_span.add_child_span(tool_span)
root_span.add_child_span(chain_span)
```

### 3. Add the inputs and outputs

Populate spans with the input and output data as well as any metadata: 

```python
# add the Inputs and Outputs to the span as dictionaries
tool_span.add_named_result(
  {"input": "search: google founded in year"}, 
  {"response": "1998"}
)

chain_span.add_named_result(
  {"input": "calculate: 2023 - 1998"}, 
  {"response": "25"}
)

llm_span.add_named_result(
  {"system": "you are a helpful assistant", 
    "input": "calculate: 2023 - 1998"}, 
  {"response": "25"}
)

root_span.add_named_result(
  {"user": "How old is google?"},
  {"response": "25 years old"}
)
```

### 4. Add metadata, status, start and end time to a Span

Any span can also have metadata, status, status messages, start and end timestamps:

```python
# add metadata to the span using .attributes
tokens_used = 284
llm_span.attributes = {"token_usage": tokens_used}

# often you want to add the same metadata to different spans
root_span.attributes = {"token_usage": tokens_used}

# add a status code and any message to any span
root_span.status_code = trace_tree.StatusCode.ERROR  # or SUCCESS
root_span.status_message = "Error: there was an error"

# add the start and end timestamp for any span, in milliseconds 
root_span.start_time_ms = 1685649600011
root_span.end_time_ms = 1685649611000
```

### 5. Log the spans to W&B Trace 

Log your span to W&B with the run.log() method. W&B will create a Trace Table and Trace Timeline for you to view in the W&B App UI.


```python
import wandb 

trace = trace_tree.WBTraceTree(root_span)
run = wandb.init(project="wandb_prompts")
run.log({"trace": trace})
run.finish()
```
### 6. View the trace
Click on the W&B run link that is generated to see the trace of your LLM on the W&B App UI.