---
slug: /guides/weave
displayed_sidebar: default
---
import { CTAButtons } from '@site/src/components/CTAButtons/CTAButtons.tsx';

# Weave

<CTAButtons colabLink="http://wandb.me/weave_colab"/>

Weave is a lightweight toolkit for tracking and evaluating LLM applications. Use W&B Weave to visualize and inspect the execution flow of your LLMs, analyze the inputs and outputs of your LLMs, view the intermediate results and securely store and manage your prompts and LLM chain configurations.

![](/images/weave/weave-hero.png)

With W&B Weave, you can:
* Log and debug language model inputs, outputs, and traces
* Build rigorous, apples-to-apples evaluations for language model use cases
* Organize all the information generated across the LLM workflow, from experimentation to evaluations to production

<!-- ## Use Cases

Software developers, prompt engineers, ML practitioners, data scientists, and other stakeholders working with LLMs need cutting-edge tools to explore and debug LLM chains and prompts with greater granularity. With W&B Weave, you can: -->



## How to get started 
Depending on your use case, explore the following resources to get started with W&B Weave:

* [Quickstart: Track inputs and outputs of LLM calls](https://wandb.github.io/weave/quickstart)
* [Build an Evaluation pipeline tutorial](https://wandb.github.io/weave/tutorial-eval)
* [Model-Based Evaluation of RAG applications tutorial](https://wandb.github.io/weave/tutorial-rag)

<a href="https://wandb.me/weave" target="_blank">
    <img className="no-zoom" src="/images/weave/weave_banner.png" alt="Building LLM apps? Try Weave" style={{display: "block", marginBottom: "15px"}} />
</a>